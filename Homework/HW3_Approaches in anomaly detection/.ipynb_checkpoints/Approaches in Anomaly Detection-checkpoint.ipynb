{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Approaches in anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('oc_514.mat')\n",
    "train = mat['x']\n",
    "xtrain = train[0,0][0]\n",
    "\n",
    "# Commented split of train and test as using k-fold cross validation approach for outlier detection.\n",
    "X_train, X_test = train_test_split(\n",
    "   xtrain, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Using Gaussian Kernel Density Estimation\n",
    "def GaussianKDE(X_train, X_test):\n",
    "    #pdf = KernelDensity(bandwidth=0.25, kernel='linear')\n",
    "    pdf = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
    "    pdf.fit(X_train)\n",
    "    resTrain = []\n",
    "    resTest = []\n",
    "    \n",
    "    for data in X_train[:,:]:\n",
    "        resTrain.append(np.exp(pdf.score_samples(data))[0])\n",
    "    \n",
    "    for data in X_test[:,:]:\n",
    "        resTest.append(np.exp(pdf.score_samples(data))[0])\n",
    "    \n",
    "    nresTrain = np.array(resTrain)\n",
    "    nresTest = np.array(resTest)\n",
    "    \n",
    "    # Assumption: \"Normal data instances occur in high probability\n",
    "    # regions of a stochastic model, while anomalies occur in the low\n",
    "    # probability regions of the stochastic model.\"\n",
    "    return nresTrain[nresTrain <= 0.05].size, nresTest[nresTest <= 0.05].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 81.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cross Validation using K-Fold so that every data sample contributes in Training\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "X_train = xtrain\n",
    "#print X_train.shape\n",
    "n_folds = 5\n",
    "kf = KFold(420, n_folds)\n",
    "error_train = error_test = 0\n",
    "for train, test in kf:\n",
    "    #print train,test\n",
    "    n_error_train, n_error_test = GaussianKDE(X_train[train,:], X_train[test,:])\n",
    "    error_test += n_error_test\n",
    "    error_train += n_error_train\n",
    "    \n",
    "print float(error_train)/n_folds , float(error_test)/n_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 105\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test set in the ratio of 3:1\n",
    "\n",
    "n_error_train, n_error_test = GaussianKDE(X_train[:315,:], X_train[315:,:])\n",
    "\n",
    "\n",
    "print n_error_train, n_error_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of KDE for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KDE we got 81 anomalies out of 420 samples. This performs poorly as the number of anomalies is higher than the expected number of 183 as defined in the webpage of the dataset. Kernel Density Estimation (KDE) can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions which is the case with our dataset as it has 278 features.\n"
     ]
    }
   ],
   "source": [
    "print \"Using KDE we got\", (error_train + error_test)/n_folds , \"anomalies out of 420 samples. This performs poorly as the number of anomalies is higher than the expected number of 183 as defined in the webpage of the dataset. Kernel Density Estimation (KDE) can be performed in any number of dimensions, though in practice the curse of dimensionality causes its performance to degrade in high dimensions which is the case with our dataset as it has 278 features.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection using One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xtrain = train[0,0][0]\n",
    "def OutlierUsingOneClassSVM(X_train, X_test):\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.font_manager\n",
    "    from sklearn import svm\n",
    "    import scipy.io as sio\n",
    "\n",
    "    #xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "\n",
    "    # Generate train data\n",
    "    #X = 0.3 * np.random.randn(100, 2)\n",
    "    #X_train = np.r_[X + 2, X - 2]\n",
    "    # Generate some regular novel observations\n",
    "    #X = 0.3 * np.random.randn(20, 2)\n",
    "    #X_test = np.r_[X + 2, X - 2]\n",
    "    # Generate some abnormal novel observations\n",
    "    #X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "\n",
    "    # fit the model\n",
    "    #clf = svm.OneClassSVM(nu=0.2, kernel=\"rbf\", gamma=0.1)\n",
    "    clf = svm.OneClassSVM(nu=0.435, kernel=\"linear\", gamma=0.1)\n",
    "    clf.fit(X_train)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    #y_pred_outliers = clf.predict(X_outliers)\n",
    "    n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "    n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "    #n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "    # Plots\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "#     Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "\n",
    "#     plt.title(\"Novelty Detection\")\n",
    "#     plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\n",
    "#     a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\n",
    "#     plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n",
    "\n",
    "#     b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "#     b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "#     # #c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "#     plt.axis('tight')\n",
    "#     plt.xlim((-5, 5))\n",
    "#     plt.ylim((-5, 5))\n",
    "#     plt.legend([a.collections[0], b1, b2], #, c],\n",
    "#                [\"learned frontier\", \"training observations\",\n",
    "#                 \"new regular observations\", \"new abnormal observations\"],\n",
    "#                loc=\"upper left\",\n",
    "#                prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "#     plt.xlabel(\n",
    "#         \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "#         \"errors novel abnormal:\"\n",
    "#         % (n_error_train, n_error_test)) #, n_error_outliers))\n",
    "#     plt.show()\n",
    "\n",
    "    return n_error_train, n_error_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420L, 278L)\n",
      "146.8 37.0\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation using K-Fold so that every data sample contributes in Training\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "X_train = xtrain\n",
    "print X_train.shape\n",
    "n_folds = 5\n",
    "kf = KFold(420, n_folds)\n",
    "error_train = error_test = 0\n",
    "for train, test in kf:\n",
    "    #print train,test\n",
    "    n_error_train, n_error_test = OutlierUsingOneClassSVM(X_train[train,:], X_train[test,:])\n",
    "    #print error_train, error_test\n",
    "    #print n_error_train, n_error_test\n",
    "    error_train += n_error_train\n",
    "    error_test += n_error_test\n",
    "    \n",
    "print float(error_train)/n_folds, float(error_test)/n_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 37\n"
     ]
    }
   ],
   "source": [
    "# Split the train and test set in the ratio of 4:1\n",
    "n_error_train, n_error_test = OutlierUsingOneClassSVM(X_train[:336,:], X_train[336:,:])\n",
    "\n",
    "print n_error_train, n_error_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of One Class SVM for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using One Class SVM we got 184 anomalies out of 420 samples. This performs well after tuning the kernel to be linear and with appropriate parameters as the number of anomalies matches the expected number of 183 as defined in the webpage of the dataset.\n"
     ]
    }
   ],
   "source": [
    "print \"Using One Class SVM we got\", (n_error_train + n_error_test) , \"anomalies out of 420 samples. This performs well after tuning the kernel to be linear and with appropriate parameters as the number of anomalies matches the expected number of 183 as defined in the webpage of the dataset.\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "\n",
    "# def euclideanDistance(point1, point2):\n",
    "#     # Computes the Euclidean distance between two objects or points. \n",
    "#     # compute RMSE (root mean squared error)\n",
    "#     difference = point1 - point2\n",
    "#     rmse = (sum(map(lambda x: x**2, difference)) / len(difference))**0.5\n",
    "#     return rmse\n",
    "\n",
    "def euclideanDistance(instance1, instance2):\n",
    "    \"\"\"Computes the distance between two instances. Instances should be tuples of equal length.\n",
    "    Returns: Euclidean distance\n",
    "    Signature: ((attr_1_1, attr_1_2, ...), (attr_2_1, attr_2_2, ...)) -> float\"\"\"\n",
    "    def detect_value_type(attribute):\n",
    "        \"\"\"Detects the value type (number or non-number).\n",
    "        Returns: (value type, value casted as detected type)\n",
    "        Signature: value -> (str or float type, str or float value)\"\"\"\n",
    "        from numbers import Number\n",
    "        attribute_type = None\n",
    "        if isinstance(attribute, Number):\n",
    "            attribute_type = float\n",
    "            attribute = float(attribute)\n",
    "        else:\n",
    "            attribute_type = str\n",
    "            attribute = str(attribute)\n",
    "        return attribute_type, attribute\n",
    "    # check if instances are of same length\n",
    "    if len(instance1) != len(instance2):\n",
    "        raise AttributeError(\"Instances have different number of arguments.\")\n",
    "    # init differences vector\n",
    "    differences = [0] * len(instance1)\n",
    "    # compute difference for each attribute and store it to differences vector\n",
    "    for i, (attr1, attr2) in enumerate(zip(instance1, instance2)):\n",
    "        type1, attr1 = detect_value_type(attr1)\n",
    "        type2, attr2 = detect_value_type(attr2)\n",
    "        # raise error is attributes are not of same data type.\n",
    "        if type1 != type2:\n",
    "            raise AttributeError(\"Instances have different data types.\")\n",
    "        if type1 is float:\n",
    "            # compute difference for float\n",
    "            differences[i] = attr1 - attr2\n",
    "        else:\n",
    "            # compute difference for string\n",
    "            if attr1 == attr2:\n",
    "                differences[i] = 0\n",
    "            else:\n",
    "                differences[i] = 1\n",
    "    # compute RMSE (root mean squared error)\n",
    "    rmse = (sum(map(lambda x: x**2, differences)) / len(differences))**0.5\n",
    "    return rmse\n",
    "\n",
    "class LOF:\n",
    "    \"\"\"Helper class for performing LOF computations and instances normalization.\"\"\"\n",
    "    def __init__(self, instances, normalize=True, distance_function=euclideanDistance):\n",
    "        self.instances = instances\n",
    "        self.normalize = normalize\n",
    "        self.distance_function = distance_function\n",
    "        if normalize:\n",
    "            self.normalize_instances()\n",
    "\n",
    "    def compute_instance_attribute_bounds(self):\n",
    "        min_values = [float(\"inf\")] * len(self.instances[0]) #n.ones(len(self.instances[0])) * n.inf \n",
    "        max_values = [float(\"-inf\")] * len(self.instances[0]) #n.ones(len(self.instances[0])) * -1 * n.inf\n",
    "        for instance in self.instances:\n",
    "            min_values = tuple(map(lambda x,y: min(x,y), min_values,instance)) #n.minimum(min_values, instance)\n",
    "            max_values = tuple(map(lambda x,y: max(x,y), max_values,instance)) #n.maximum(max_values, instance)\n",
    "        self.max_attribute_values = max_values\n",
    "        self.min_attribute_values = min_values\n",
    "            \n",
    "    def normalize_instances(self):\n",
    "        \"\"\"Normalizes the instances and stores the infromation for rescaling new instances.\"\"\"\n",
    "        if not hasattr(self, \"max_attribute_values\"):\n",
    "            self.compute_instance_attribute_bounds()\n",
    "        new_instances = []\n",
    "        for instance in self.instances:\n",
    "            new_instances.append(self.normalize_instance(instance)) # (instance - min_values) / (max_values - min_values)\n",
    "        self.instances = new_instances\n",
    "        \n",
    "    def normalize_instance(self, instance):\n",
    "        return tuple(map(lambda value,max,min: (value-min)/(max-min) if max-min > 0 else 0, \n",
    "                         instance, self.max_attribute_values, self.min_attribute_values))\n",
    "        \n",
    "    def local_outlier_factor(self, min_pts, instance):\n",
    "        \"\"\"The (local) outlier factor of instance captures the degree to which we call instance an outlier.\n",
    "        min_pts is a parameter that is specifying a minimum number of instances to consider for computing LOF value.\n",
    "        Returns: local outlier factor\n",
    "        Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float\"\"\"\n",
    "        if self.normalize:\n",
    "            instance = self.normalize_instance(instance)\n",
    "        return local_outlier_factor(min_pts, instance, self.instances, distance_function=self.distance_function)\n",
    "\n",
    "def k_distance(k, instance, instances, distance_function=euclideanDistance):\n",
    "    #TODO: implement caching\n",
    "    \"\"\"Computes the k-distance of instance as defined in paper. It also gatheres the set of k-distance neighbours.\n",
    "    Returns: (k-distance, k-distance neighbours)\n",
    "    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> (float, ((attr_j_1, ...),(attr_k_1, ...), ...))\"\"\"\n",
    "    distances = {}\n",
    "    for instance2 in instances:\n",
    "        distance_value = distance_function(instance, instance2)\n",
    "        if distance_value in distances:\n",
    "            distances[distance_value].append(instance2)\n",
    "        else:\n",
    "            distances[distance_value] = [instance2]\n",
    "    distances = sorted(distances.items())\n",
    "    neighbours = []\n",
    "    [neighbours.extend(n[1]) for n in distances[:k]]\n",
    "    return distances[k - 1][0], neighbours\n",
    "\n",
    "def reachability_distance(k, instance1, instance2, instances, distance_function=euclideanDistance):\n",
    "    \"\"\"The reachability distance of instance1 with respect to instance2.\n",
    "    Returns: reachability distance\n",
    "    Signature: (int, (attr_1_1, ...),(attr_2_1, ...)) -> float\"\"\"\n",
    "    (k_distance_value, neighbours) = k_distance(k, instance2, instances, distance_function=distance_function)\n",
    "    return max([k_distance_value, distance_function(instance1, instance2)])\n",
    "\n",
    "def local_reachability_density(min_pts, instance, instances, **kwargs):\n",
    "    \"\"\"Local reachability density of instance is the inverse of the average reachability \n",
    "    distance based on the min_pts-nearest neighbors of instance.\n",
    "    Returns: local reachability density\n",
    "    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float\"\"\"\n",
    "    (k_distance_value, neighbours) = k_distance(min_pts, instance, instances, **kwargs)\n",
    "    reachability_distances_array = [0]*len(neighbours) #n.zeros(len(neighbours))\n",
    "    for i, neighbour in enumerate(neighbours):\n",
    "        reachability_distances_array[i] = reachability_distance(min_pts, instance, neighbour, instances, **kwargs) \n",
    "    return len(neighbours) / sum(reachability_distances_array)\n",
    "\n",
    "def local_outlier_factor(min_pts, instance, instances, **kwargs):\n",
    "    \"\"\"The (local) outlier factor of instance captures the degree to which we call instance an outlier.\n",
    "    min_pts is a parameter that is specifying a minimum number of instances to consider for computing LOF value.\n",
    "    Returns: local outlier factor\n",
    "    Signature: (int, (attr1, attr2, ...), ((attr_1_1, ...),(attr_2_1, ...), ...)) -> float\"\"\"\n",
    "    (k_distance_value, neighbours) = k_distance(min_pts, instance, instances, **kwargs)\n",
    "    instance_lrd = local_reachability_density(min_pts, instance, instances, **kwargs)\n",
    "    lrd_ratios_array = [0]* len(neighbours)\n",
    "    for i, neighbour in enumerate(neighbours):\n",
    "        instances_without_instance = set(instances)\n",
    "        instances_without_instance.discard(neighbour)\n",
    "        neighbour_lrd = local_reachability_density(min_pts, neighbour, instances_without_instance, **kwargs)\n",
    "        lrd_ratios_array[i] = neighbour_lrd / instance_lrd\n",
    "    return sum(lrd_ratios_array) / len(neighbours)\n",
    "\n",
    "def outliers(k, instances, **kwargs):\n",
    "    \"\"\"Simple procedure to identify outliers in the dataset.\"\"\"\n",
    "    instances_value_backup = instances\n",
    "    outliers = []\n",
    "    for i, instance in enumerate(instances_value_backup):\n",
    "        instances = list(instances_value_backup)\n",
    "        instances.remove(instance)\n",
    "        l = LOF(instances, **kwargs)\n",
    "        value = l.local_outlier_factor(k, instance)\n",
    "        if value > 1:\n",
    "            outliers.append({\"lof\": value, \"instance\": instance, \"index\": i})\n",
    "    outliers.sort(key=lambda o: o[\"lof\"], reverse=True)\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = X_train[:336,:]\n",
    "lof = LOF(train)\n",
    "\n",
    "for instance in X_train[336:,:]:\n",
    "    value = lof.local_outlier_factor(10, instance)\n",
    "    print value, instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
